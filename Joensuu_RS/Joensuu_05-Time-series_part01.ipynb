{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Time series: pre-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Author(s):** Quentin Yeche, Kenji Ose, Dino Ienco - [UMR TETIS](https://umr-tetis.fr) / [INRAE](https://www.inrae.fr/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook we will introduce the time dimension into our exploration of data cubes. We will also cover handling masks, and specifically cloud cover masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import libraries\n",
    "\n",
    "As usual, we import all the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STAC access\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# xarrays\n",
    "import xarray as xr\n",
    "\n",
    "# library for turning STAC objects into xarrays\n",
    "import stackstac\n",
    "\n",
    "# visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# miscellanous\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting a Sentinel-2 images time series\n",
    "\n",
    "### 3.1. Request on STAC catalog\n",
    "\n",
    "As a practical use case let's consider that we have identified the STAC Collection we're interested in (see [this notebook](Joensuu_01-STAC.ipynb) for a refresher), and we also have an area of interest defined as a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aoi_bounds = (3.875107329166124, 43.48641456618909, 4.118824575734205, 43.71739887308995)\n",
    "\n",
    "# retrieving the relevant STAC Item\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "today = date.today()\n",
    "last_month = today.replace(month=today.month-1).strftime('%Y-%m')\n",
    "time_range = f\"2020-01-01/{last_month}\"\n",
    "search = catalog.search(\n",
    "    collections=['sentinel-2-l2a'],\n",
    "    datetime=time_range,\n",
    "    bbox=aoi_bounds\n",
    ")\n",
    "items = search.item_collection()\n",
    "print(f\"{len(items)} items found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, we display the assets description of the first item (or image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "item = items[0]\n",
    "assets = [asset.title for asset in item.assets.values()]\n",
    "descriptions = pd.DataFrame(assets, \n",
    "                            columns=['Description'], \n",
    "                            index=pd.Series(item.assets.keys(), \n",
    "                                            name='asset_key'))\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'> **IMPORTANT:** The URLs that we obtain from the STAC Catalog are not valid indefinitely. They expire after about 30 minutes. If you see an error in the notebook, it is likely because the URL that you obtained by running the first few cells has now expired.</span> If that happens you should be able to just run the notebook again from the top to get a new URL. You can get longer-lasting URLs by signing up for Planetary Computer (which is free at the time of writing this notebook). More info [here](https://planetarycomputer.microsoft.com/docs/concepts/sas/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Creating a datacube from STAC search results\n",
    "\n",
    "This process is very similar to what we did to create a DataArray in the beginning of the [previous notebook](Joensuu_03-Xarray.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bands = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12']\n",
    "FILL_VALUE = 2**16-1\n",
    "array = stackstac.stack(\n",
    "                    items,\n",
    "                    assets = bands,\n",
    "                    resolution=10,\n",
    "                    dtype=\"uint16\",\n",
    "                    fill_value=FILL_VALUE,\n",
    "                    bounds_latlon=aoi_bounds,\n",
    "                    )\n",
    "array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in a few lines of code we have created a DataArray which data has a size in the tens of gigabytes. The idea of lazy computations and not downloading data until necessary starts to make a lot more sense with these sizes. When we also consider the fact our area of interest is only about 5% of a full Sentinel-2 tile, the ability to download only part of a tile also becomes very important. If we had to download the entire tiles before creating this array the volume of data would probably be measured in terabytes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Checking for invalid values\n",
    "\n",
    "Before moving on, it is always a good idea to check the validity of the DataArray we just created. There are two main things to check for:\n",
    " - The presence of fill values. As described in the [previous notebook](Joensuu_03-Xarray.ipynb), the fill value is used by `stackstac.stack` in order to replace missing STAC Assets.\n",
    " - The presence of NODATA values. In our case (Sentinel-2) pixel values of 0 are used to flag that the pixel does not contain valid data.\n",
    "\n",
    "#### 3.3.1. Checking the presence of fill values\n",
    "\n",
    "Of course, checking for these values necessitate to download the data. Thus we can really only look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we're only taking the first 4 dates\n",
    "sample = array.isel(time=slice(0,4))\n",
    "fill_values = xr.where(sample == FILL_VALUE, 1, 0).sum().values\n",
    "\n",
    "print(f\"Data contains {fill_values} fill value pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we see no fill values. Indeed we have no missing Asset in any of our Items. Our collection of STAC Items is homogenous, all Sentinel-2 Items contain the right Assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Checking the presence of NoData\n",
    "\n",
    "There are two situations in which NODATA pixels can appear. The first is extents which do not strictly overlap. Satellite images are almost always distributed as multi-dimensional array with a rectangular shape. If the bounds do not exactly match then the image is completed with NODATA values to make up a rectangular shape.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"resources/extent_nodata.png\" width=\"30%\" alt=\"STAC Item diagram\">\n",
    "  <figcaption>Sentinel-2 image with NODATA values\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The second situation where NODATA values can appear is when some pixels are flagged as invalid despite being inside the boundaries of the area being captured by the captor. Typically the amount of such invalid pixels is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_data = xr.where(sample == 0, 1, 0).sum().values\n",
    "print(f\"{no_data/sample.size:.1%} of values are NODATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that almost half of the pixels in the sample are invalid. So it appears that at least some our images do not fully overlap with our area of interest.\n",
    "\n",
    "#### 3.3.3. Checking through `xarray` coordinates\n",
    "\n",
    "The first thing we can look at is the `s2:nodata_pixel_percentage`. It gives us the percentage of pixels with NODATA in the tile for each date.\n",
    "\n",
    "<span style='color:red'> **IMPORTANT:**</span> Planetary Computer provides a few metadata properties along with the data.\n",
    " For Sentinel-2 the most useful include:\n",
    " - `s2:nodata_pixel_percentage`\n",
    " - `s2:water_percentage`\n",
    " - `s2:vegetation_percentage`\n",
    " - `eo:cloud_cover`\n",
    " - `s2:high_proba_clouds_percentage`\n",
    " - `s2:cloud_shadow_percentage`\n",
    " \n",
    "However <span style='color:red'> **these values were computed from the whole tile (an area of up to 110 x 110 km²), and they are NOT updated when only a sub-section of a tile is considered or downloaded**</span>. The values can still be used as a proxy or rough estimate without having to download the data. It is particularly useful when gathering data in order to decide what to keep or throw out. But be careful not to confuse them with the actual value which can be computed from a sub-section of a tile. For instance a Sentinel-2 tile could have a low value for `eo:cloud_cover`, indicating that there are relatively few clouds in the whole tile. But that doesn't mean that our area of interest could not contain some of these few clouds, leading to unusable data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[f'{pct:.2f} %' for pct in sample['s2:nodata_pixel_percentage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half the dates have more than 85% of their pixels labelled as NODATA, while the others have few to none. Let's see if this behavior applies to the whole data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array['s2:nodata_pixel_percentage'].plot.hist()\n",
    "plt.title('nodata pixel percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we are dealing with two different types Sentinel-2 images, one of which only has a small amount of valid values. We can know more by looking at the `sat:relative_orbit` coordinate of our array. It gives us the number of the orbit during which the image was captured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array['sat:relative_orbit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were captured with two different orbits: 8 and 108. Let's plot the same histogram as before, but discriminating between the two orbits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array.sel(time=array.time['sat:relative_orbit']==8)['s2:nodata_pixel_percentage'].plot.hist()\n",
    "array.sel(time=array.time['sat:relative_orbit']==108)['s2:nodata_pixel_percentage'].plot.hist()\n",
    "\n",
    "plt.legend(['Orbit 8', 'Orbit 108'])\n",
    "plt.title('NODATA values percentage per orbit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, one of the two orbits is mainly at fault: orbit 108. However, let's keep in mind that these values we're looking at from `s2:nodata_pixel_percentage` are for the whole image which corresponds to a much bigger area than ours. But at this point there is little chance that our area happens to overlap with this small portion of valid pixels.\n",
    "\n",
    "We can check it with a similar test to what we did for the fill values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting from our sample the dates with orbit 108\n",
    "sample_108 = sample.sel(time=sample.time['sat:relative_orbit']==108)\n",
    "\n",
    "# counting the number of NODATA pixels as a percentage\n",
    "# of overall pixel count\n",
    "nodata_108 = xr.where(sample_108==0,1,0).sum().values\n",
    "print(f\"{nodata_108/sample_108.size:.1%} of pixels in the array are NODATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our area of interest falls almost entirely outside of the area of validity for the images with orbit 108. Thus we need to simply restrict our study to orbit 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array = array.sel(time=array.time['sat:relative_orbit']==8)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous histograms we could see that even with orbit 8 there were a few values with higher than average amounts of invalid pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array.sel(time=array.time['s2:nodata_pixel_percentage']>5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to take a look at these dates and their data to decide whether they should be kept. Let's just decide to remove them, as this is only 3 dates out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "array = array.sel(time=array.time['s2:nodata_pixel_percentage'] <5)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4. Extra: The relationship between Sentinel-2 tiles and orbits\n",
    "\n",
    "Sentinel-2 uses the concept of tiles. A Sentinel-2 tile delimits an area of 100x100 km². Each tile has a unique code. The tile that our area of interest falls in is the tile 31TEJ (often times the T is added in front: T31TEJ). \n",
    "\n",
    "**Note:** As an aside this code can be found in `s2:mgrs_tile` and as part of the `s2:granule_id` coordinate of our array. More information on the naming conventions used by Sentinel-2 can be found [here](https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/naming-convention#:~:text=The%20top%2Dlevel%20SENTINEL%2D2,separated%20by%20an%20underscore%20(_).&text=The%20Mission%20Identifier%20(MMM)%20denotes,for%20the%20SENTINEL%2D2B%20instrument.).\n",
    "\n",
    "In order to capture an image in the area delimited by the tile 31TEJ the satellite has to fly over the area along an orbit. For tile 31TEJ this orbit is orbit 8. But in order to ensure that there exists overlap between the tiles, data is actually captured with some overlap in a 110x110 km² area. \n",
    "\n",
    "Here is now the explanation for the two orbits. The images we see with orbit 108 actually correspond to the acquisition for a different, neighboring tile 31TJG. But since there is overlap between the two images, a small area near the edge of tile 31TEJ is covered by the acquisitions made with orbit 108.\n",
    "\n",
    "**Note:** A useful interactive map of the Sentinel-2 tiling grid is available [here](https://eatlas.org.au/data/uuid/f7468d15-12be-4e3f-a246-b2882a324f59) (it can take a few seconds for the map to load in completely). It clearly shows the different tiles, their codes and the overlap between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on the time dimension\n",
    "\n",
    "In the [previous notebook](Joensuu_03-Xarray.ipynb) we covered how to manipulate Xarrays. Now that we have introduced multiple dates in our time dimension, there are a few more tools which are useful when dealing with temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pandas dates and inexact matching\n",
    "\n",
    "There are situations in which having to match the exact values is not desirable. Date objects are usually given with a high degree of precision, and it is not practical having to always specify hours minutes and seconds in situations where the date would suffice. There are a few tools which help alleviate this annoyance.\n",
    "\n",
    "The first is that Xarray uses the same logic as pandas for its date indexes. Even though the values are datetime64 objects, they can be selected with regular strings with a lot of convenience. For instance \"`2023`\" will correctly match all datetime64 objects between `2023-01-01T00:00:00` and `2023-12-31T23:59:59`, and `2023-01` will correctly match all datetime64 objects of the January 2023, etc.\n",
    "\n",
    "The other tool is the ability of the `sel` method to select inexact matches. This behavior is not limited to time dimensions, but it is particularly applicable for dates. By default, `sel` only matches exact values. However the `method` parameter can enable inexact matches:\n",
    " - `pad` or `ffill` will match the nearest value below\n",
    " - `backfill` or `bfill` will match the nearest value above\n",
    " - `nearest` will match the nearest value\n",
    "\n",
    " Another parameter of `sel` is `tolerance`. It allows to set a limit on the distance between the specified value and a possible match. For dates it can used with a `timedelta64` objet or a string like `'5D'` (5 days), `'5M3W'` (5 months 3 weeks), `'2Y15h34s'`(2 years 15 hours and 34 seconds), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "few_dates = array.isel(time=slice(3,7)).time\n",
    "print(f\"Dates:\\n {few_dates.values}\")\n",
    "\n",
    "# does not match the last date as it is in February\n",
    "print(f\"\\n2020-01: \\n {few_dates.sel(time='2020-01').values}\")\n",
    "\n",
    "# matches 2020-01-18 as it is the nearest\n",
    "print(f\"\\nnearest 2020-01-20 \\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-20', method='nearest').values}\")\n",
    "\n",
    "# matches 2020-01-23 as it is the nearest date after 2020-01-20\n",
    "print(f\"\\nbackfill 2020-01-20\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-20', method='backfill').values}\")\n",
    "\n",
    "# matches 2020-01-18 as it is the nearest date before 2020-01-22\n",
    "print(f\"\\npad 2020-01-22 \\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-22', method='pad').values}\")\n",
    "\n",
    "# 2020-01-23 is treated as 2020-01-23T00:00:00\n",
    "# which comes before 2020-01-23T10:42:29.0240\n",
    "# thus 2020-01-18 is the correct match\n",
    "print(f\"\\npad 2023-01-23\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-23', method='pad').values}\")\n",
    "\n",
    "print(f\"\\npad 2023-01-23 with tolerance 4 days 14 hours\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-23', method='pad', tolerance='4D14h').values}\")\n",
    "\n",
    "# this one would fail to get a match and give an error\n",
    "#print(f\"\\npad 2023-01-22 with tolerance 4 days 13 hours and 5 seconds\\n\",\n",
    "#      f\"{few_dates.sel(time='2020-01-23', method='pad', tolerance='4D13h5s').values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing inherited from pandas is the `dt` accessor. It allows accessing specific components of a datetime64 object. The full list of components can be found [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#time-date-components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all the data from February for all the years\n",
    "print(array.sel(time=(array.time.dt.month == 2)).time)\n",
    "print('\\n===\\n')\n",
    "# all the data captured on the 4th day of a month\n",
    "print(array.sel(time=(array.time.dt.day == 4)).time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Resampling and other operations\n",
    "#### 4.2.1. Analysing acquisition frequency\n",
    "\n",
    "What follows are a few examples of operations on dates in order to extract useful information. They can be instrumental in refining a time series. It is technically possible to do more complex operations like upsampling and interpolation, but they will not be covered here.\n",
    "\n",
    "First, we create a funtion to to format axes for dates as a fairly readable year/month standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def date_format(ax, month='%B'):\n",
    "    import matplotlib.dates as mdates\n",
    "    # adding locations and formatting for months\n",
    "    # on the x axis\n",
    "    loc = mdates.MonthLocator(bymonth=(1,4,7,10))\n",
    "    major_fmt = mdates.ConciseDateFormatter(loc,\n",
    "                formats=['%Y', month, '%d', '%H:%M', '%H:%M', '%S.%f']\n",
    "                                            )\n",
    "    ax.xaxis.set_major_formatter(major_fmt)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "    # adding minor ticks for months without a label\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate and plot the number of days between each acquisition to see if there any gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,1))\n",
    "ax.eventplot(array.time)\n",
    "ax.set_title(\"Chronogram of acquisitions\")\n",
    "\n",
    "date_format(ax, month='%b')\n",
    "\n",
    "# removing y axis ticks as they are meaningless\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "\n",
    "# plotting as a histogram\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, sharex=True)\n",
    "ax1.hist(array.time.diff(dim='time').dt.round('1D').dt.days)\n",
    "ax2.hist(array.time.diff(dim='time').dt.round('1D').dt.days)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "ax1.set_ylim(150,250)\n",
    "ax2.set_ylim(0,16)\n",
    "\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "# adding cut-out slanted lines to signify a cut in the y axis\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax1.set_title('Number of days between two acquisitions')\n",
    "ax2.set_xlabel('Number of days')\n",
    "# supylabel allows us to name both y axes together\n",
    "fig.supylabel(\"Number of occurences\")\n",
    "plt.show()\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can count the number of acquisitions per month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# resampling and counting for each month\n",
    "# using MS (Month Start) as we prefer using the first day\n",
    "# of the month as a reference point instead of the last\n",
    "# acquisition date of the month\n",
    "ar = array.time.resample(time='MS').count()\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "fig, ax = plt.subplots(figsize=(13,4))\n",
    "ar.plot.scatter(ax=ax, x='time')\n",
    "ax.set_ylabel('Acquisition count')\n",
    "ax.set_title(\"Number of acquisitions per month\")\n",
    "\n",
    "# setting y ticks manually to avoid non-integer values being used\n",
    "ax.set_yticks(range(ar.min().values, ar.max().values+1))\n",
    "\n",
    "date_format(ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2.2. Analysing variations of cloud cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# plotting the average cloud cover and vegetation for the acquistion for each month\n",
    "# they are very negatively correlated since 'cloudy' and 'vegetation' \n",
    "# are two exclusive classes for a pixel\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "array['eo:cloud_cover'].resample(time='M').mean().plot(ax=ax, color='slategrey')\n",
    "#array['s2:vegetation_percentage'].resample(time='M').mean().plot(ax=ax, color='forestgreen')\n",
    "ax.legend(['cloud cover'])#, 'vegetation'])\n",
    "ax.set_ylabel('percentage')\n",
    "ax.set_title('Average cloud cover percentage in the tile per month')\n",
    "\n",
    "date_format(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
